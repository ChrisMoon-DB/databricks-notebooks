{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f1686f9-2220-459b-9bf9-4040f011b898",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Privacy and Security in Databricks\n",
    "\n",
    "This notebook demonstrates data privacy and security features in Databricks Unity Catalog.\n",
    "\n",
    "## Topics:\n",
    "1. **RBAC** - Role-Based Access Control\n",
    "2. **Views** - Dynamic, Restricted, and Materialized\n",
    "3. **Data Hashing** - Irreversible anonymization\n",
    "4. **Data Masking** - Format-preserving obfuscation\n",
    "5. **Row Filtering** - Scope access by attributes\n",
    "6. **Tokenization** - Reversible token replacement\n",
    "7. **ABAC** - Attribute-Based Access Control\n",
    "8. **Encryption** - Protect data at rest and in transit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a83aa2b-f57e-425c-8c25-ebb29d9a0b62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuration\n",
    "\n",
    "Set your demo preferences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f69240bf-d5e2-4896-afb8-6016d12139f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Demo Configuration\n",
    "USE_TEMP_TABLES = True  # Recommended for demos - auto-cleanup on session end\n",
    "CREATE_CATALOG_SCHEMA = False  # Recommended for demos - auto-cleanup on session end. If disabled and the catalog and schema doesn't exist, code will panic.\n",
    "\n",
    "# Catalog and schema names\n",
    "CATALOG = \"main\"\n",
    "HR_SCHEMA = \"hr\"\n",
    "CUSTOMERS_SCHEMA = \"customers\"\n",
    "RETAIL_SCHEMA = \"retail\"\n",
    "GOVERNANCE_SCHEMA = \"governance\"\n",
    "\n",
    "print(f\"‚úì Configuration loaded\")\n",
    "print(f\"  ‚Üí Temporary tables: {USE_TEMP_TABLES}\")\n",
    "print(f\"  ‚Üí Catalog: {CATALOG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9edda7ce-3cb8-489d-878a-0e27bfe5e16b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries and define helper functions\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def get_table_type():\n",
    "    return \"TEMPORARY\" if USE_TEMP_TABLES else \"\"\n",
    "\n",
    "def func_name(name):\n",
    "    return name if USE_TEMP_TABLES else f\"{CATALOG}.{GOVERNANCE_SCHEMA}.{name}\"\n",
    "\n",
    "def view_name(schema, name):\n",
    "    return name if USE_TEMP_TABLES else f\"{CATALOG}.{schema}.{name}\"\n",
    "\n",
    "def table_name(schema, name):\n",
    "    return name if USE_TEMP_TABLES else f\"{CATALOG}.{schema}.{name}\"\n",
    "\n",
    "print(\"‚úì Libraries and helpers loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11f8b3cd-a7d7-4938-b9c8-b03f5dce005c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ENVIRONMENT SETUP\n",
    "# ============================================================================\n",
    "# This notebook is called via %run from the main demo notebook\n",
    "# Configuration is passed from the calling notebook\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "print(\"Setting up demo environment...\")\n",
    "print(f\"‚Üí Mode: {'Temporary tables' if USE_TEMP_TABLES else 'Permanent tables'}\")\n",
    "\n",
    "# Only create schemas if using permanent tables\n",
    "if CREATE_CATALOG_SCHEMA and not USE_TEMP_TABLES:\n",
    "    for schema in [HR_SCHEMA, CUSTOMERS_SCHEMA, RETAIL_SCHEMA, GOVERNANCE_SCHEMA]:\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{schema}\")\n",
    "    print(\"‚úì Schemas created\")\n",
    "\n",
    "# Helper function\n",
    "def get_create_stmt(table_name, schema, columns):\n",
    "    if USE_TEMP_TABLES:\n",
    "        return f\"CREATE OR REPLACE TEMP VIEW {table_name} AS SELECT {columns}\"\n",
    "    else:\n",
    "        return f\"CREATE OR REPLACE TABLE {CATALOG}.{schema}.{table_name} ({columns})\"\n",
    "\n",
    "# HR employee_info table or temp view\n",
    "if USE_TEMP_TABLES:\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW employee_info AS\n",
    "        SELECT * FROM VALUES\n",
    "            (1, 'David Wells', 100000.00, '123-45-6789'),\n",
    "            (2, 'Chris Moon', 120000.00, '234-56-7890'),\n",
    "            (3, 'Jane Doe', 95000.00, '345-67-8901'),\n",
    "            (4, 'John Smith', 110000.00, '456-78-9012')\n",
    "        AS t(id, name, salary, ssn)\n",
    "    \"\"\")\n",
    "else:\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE {CATALOG}.{HR_SCHEMA}.employee_info (\n",
    "            id INT, name STRING, salary DECIMAL(10, 2), ssn STRING)\n",
    "    \"\"\")\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO {CATALOG}.{HR_SCHEMA}.employee_info VALUES\n",
    "            (1, 'David Wells', 100000.00, '123-45-6789'),\n",
    "            (2, 'Chris Moon', 120000.00, '234-56-7890'),\n",
    "            (3, 'Jane Doe', 95000.00, '345-67-8901'),\n",
    "            (4, 'John Smith', 110000.00, '456-78-9012')\n",
    "    \"\"\")\n",
    "\n",
    "# Customers customer_info table or temp view\n",
    "if USE_TEMP_TABLES:\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW customer_info AS\n",
    "        SELECT * FROM VALUES\n",
    "            (1, 'david.wells@databricks.com', 'David Wells', '2025-01-01'),\n",
    "            (2, 'chris.moon@databricks.com', 'Chris Moon', '2025-02-01'),\n",
    "            (3, 'jane.doe@example.com', 'Jane Doe', '2025-03-15'),\n",
    "            (4, 'john.smith@example.com', 'John Smith', '2025-04-20')\n",
    "        AS t(id, email, name, created_at)\n",
    "    \"\"\")\n",
    "else:\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE {CATALOG}.{CUSTOMERS_SCHEMA}.customer_info (\n",
    "            id INT, email STRING, name STRING, created_at DATE)\n",
    "    \"\"\")\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO {CATALOG}.{CUSTOMERS_SCHEMA}.customer_info VALUES\n",
    "            (1, 'david.wells@databricks.com', 'David Wells', '2025-01-01'),\n",
    "            (2, 'chris.moon@databricks.com', 'Chris Moon', '2025-02-01'),\n",
    "            (3, 'jane.doe@example.com', 'Jane Doe', '2025-03-15'),\n",
    "            (4, 'john.smith@example.com', 'John Smith', '2025-04-20')\n",
    "    \"\"\")\n",
    "\n",
    "# Retail customers table or temp view\n",
    "if USE_TEMP_TABLES:\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW customers AS\n",
    "        SELECT * FROM VALUES\n",
    "            (1, '123-45-6789', 'Alice Smith', 'US'),\n",
    "            (2, '234-56-7890', 'Maria Silva', 'EU'),\n",
    "            (3, '456-78-9012', 'Akira Tanaka', 'APAC'),\n",
    "            (4, '567-89-0123', 'Bob Johnson', 'US'),\n",
    "            (5, '678-90-1234', 'Emma Brown', 'EU')\n",
    "        AS t(id, ssn, name, region)\n",
    "    \"\"\")\n",
    "else:\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE {CATALOG}.{RETAIL_SCHEMA}.customers (\n",
    "            id INT, ssn STRING, name STRING, region STRING)\n",
    "    \"\"\")\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO {CATALOG}.{RETAIL_SCHEMA}.customers VALUES\n",
    "            (1, '123-45-6789', 'Alice Smith', 'US'),\n",
    "            (2, '234-56-7890', 'Maria Silva', 'EU'),\n",
    "            (3, '456-78-9012', 'Akira Tanaka', 'APAC'),\n",
    "            (4, '567-89-0123', 'Bob Johnson', 'US'),\n",
    "            (5, '678-90-1234', 'Emma Brown', 'EU')\n",
    "    \"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úì Setup Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(\"‚Üí All tables created and populated\")\n",
    "print(\"‚Üí Ready for demonstrations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37f6364c-832b-4d17-ab2b-7cf812667864",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 4. Data Masking\n",
    "\n",
    "**What is Data Masking?**\n",
    "Replaces sensitive values with obfuscated versions while maintaining format and structure.\n",
    "\n",
    "**Use Cases:**\n",
    "- Display masked SSNs (XXX-XX-6789) instead of raw values\n",
    "- Preserve formats for analytics while hiding true values\n",
    "- Automate masking by user/group with Unity Catalog\n",
    "\n",
    "**Key Functions:** `IS_ACCOUNT_GROUP_MEMBER()`, `IS_MEMBER()`, `mask()`\n",
    "\n",
    "**Important:** Fine-grained controls require serverless compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1155a94-d52c-45fb-a9f8-b653b4cd7951",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create masking function based on group membership\n",
    "spark.sql(f\"\"\"CREATE OR REPLACE {get_table_type()} FUNCTION {func_name('mask_ssn')}(ssn STRING)\n",
    "RETURNS STRING\n",
    "RETURN CASE\n",
    "    WHEN IS_ACCOUNT_GROUP_MEMBER('admin') THEN ssn\n",
    "    ELSE '***-**-****'\n",
    "END\"\"\")\n",
    "\n",
    "# Create view with masked SSN\n",
    "spark.sql(f\"\"\"CREATE OR REPLACE {get_table_type()} VIEW {view_name(RETAIL_SCHEMA, 'v_customers_masked')} AS\n",
    "SELECT id, {func_name('mask_ssn')}(ssn) AS ssn, name, region\n",
    "FROM {table_name(RETAIL_SCHEMA, 'customers')}\"\"\")\n",
    "\n",
    "print(\"Original Data:\")\n",
    "display(spark.sql(f\"SELECT * FROM {table_name(RETAIL_SCHEMA, 'customers')} LIMIT 3\"))\n",
    "\n",
    "print(\"\\nMasked Data (SSN hidden based on permissions):\")\n",
    "display(spark.sql(f\"SELECT * FROM {view_name(RETAIL_SCHEMA, 'v_customers_masked')} LIMIT 3\"))\n",
    "\n",
    "print(\"\\n‚úì SSN masked based on group membership\")\n",
    "print(\"‚úì Admins see full SSN, others see masked\")\n",
    "print(\"‚úì Format preserved for analytics\")\n",
    "print(\"\\n‚ö†Ô∏è  Update 'admin' to your admin group name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b7bbe08-26ea-4b11-8d49-bdb30dd5f14e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 5. Row-Level Filtering\n",
    "\n",
    "**What is Row Filtering?**\n",
    "Controls which records users can view by applying row-level conditions, enforced transparently at query time.\n",
    "\n",
    "**Use Cases:**\n",
    "- GDPR: Restrict EU data to EU employees only\n",
    "- Multi-tenancy: Each customer sees only their data\n",
    "- Financial segmentation: Business units see only their accounts\n",
    "- Data sharing: Curated datasets for external partners\n",
    "\n",
    "**Benefits:** Transparent enforcement ‚Ä¢ Combines with column masks ‚Ä¢ No data duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c39b25ed-5cf4-44c9-ae72-4c7b544d0fb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create row filter function based on region\n",
    "spark.sql(f\"\"\"CREATE OR REPLACE {get_table_type()} FUNCTION {func_name('filter_by_region')}(region STRING)\n",
    "RETURNS BOOLEAN\n",
    "RETURN CASE\n",
    "    -- Add the current user for demo purposes, in production you would use the groups as shown below.\n",
    "    WHEN CURRENT_USER() = '{dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()}' AND region = 'US' THEN TRUE\n",
    "    WHEN IS_MEMBER('Team_US') AND region = 'US' THEN TRUE\n",
    "    WHEN IS_MEMBER('Team_EU') AND region = 'EU' THEN TRUE\n",
    "    WHEN IS_MEMBER('Team_APAC') AND region = 'APAC' THEN TRUE\n",
    "    WHEN IS_ACCOUNT_GROUP_MEMBER('admin') THEN TRUE\n",
    "    ELSE FALSE\n",
    "END\"\"\")\n",
    "\n",
    "# Create view with row filtering\n",
    "spark.sql(f\"\"\"CREATE OR REPLACE {get_table_type()} VIEW {view_name(RETAIL_SCHEMA, 'v_customers_filtered')} AS\n",
    "SELECT id, ssn, name, region\n",
    "FROM {table_name(RETAIL_SCHEMA, 'customers')}\n",
    "WHERE {func_name('filter_by_region')}(region)\"\"\")\n",
    "\n",
    "print(\"All Data (5 rows):\")\n",
    "display(spark.sql(f\"SELECT * FROM {table_name(RETAIL_SCHEMA, 'customers')} ORDER BY id\"))\n",
    "\n",
    "print(\"\\nFiltered Data (based on user's region):\")\n",
    "display(spark.sql(f\"SELECT * FROM {view_name(RETAIL_SCHEMA, 'v_customers_filtered')} ORDER BY id\"))\n",
    "\n",
    "print(\"\\n‚úì Team_US: US records only\")\n",
    "print(\"‚úì Team_EU: EU records only\")\n",
    "print(\"‚úì Team_APAC: APAC records only\")\n",
    "print(\"‚úì Admins: All records\")\n",
    "print(\"\\n‚ö†Ô∏è  Update Team_US, Team_EU, Team_APAC to your group names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72ed2da9-744d-487e-b03e-d8e6f2c03f58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 6. Data Tokenization\n",
    "\n",
    "**What is Tokenization?**\n",
    "Substitutes sensitive values with random tokens that map back via secure vaults. Unlike hashing, tokenization is **reversible**.\n",
    "\n",
    "**Use Cases:**\n",
    "- PCI-DSS: Replace credit cards with compliant tokens\n",
    "- Testing: Provide realistic but protected test data\n",
    "- Analytics: Enable analysis without exposing PII\n",
    "- Fraud detection: Reversible for authorized investigation\n",
    "\n",
    "**Production Integration:** VGS, Basis Theory, TokenEx\n",
    "\n",
    "**Trade-offs:** ‚úì Reversible ‚Ä¢ ‚ö†Ô∏è Requires external service ‚Ä¢ ‚ö†Ô∏è Performance overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd012261-036e-4f71-b86c-74cf3b9e5727",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create tokenization functions\n",
    "spark.sql(f\"\"\"CREATE OR REPLACE {get_table_type()} FUNCTION {func_name('tokenize')}(value STRING)\n",
    "RETURNS STRING\n",
    "RETURN CONCAT('TOK-', substr(sha2(value, 256), 1, 32))\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"CREATE OR REPLACE {get_table_type()} FUNCTION {func_name('detokenize')}(token STRING, original STRING)\n",
    "RETURNS STRING\n",
    "RETURN CASE\n",
    "    WHEN CURRENT_USER() = '{dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()}' AND region = 'US' THEN original\n",
    "    WHEN IS_ACCOUNT_GROUP_MEMBER('CAN_SEE')  THEN original\n",
    "    ELSE token\n",
    "END\"\"\")\n",
    "\n",
    "# Create tokenized data as view or table\n",
    "if USE_TEMP_TABLES:\n",
    "    spark.sql(f\"\"\"CREATE OR REPLACE TEMP VIEW customers_tokenized AS\n",
    "    SELECT id, {func_name('tokenize')}(ssn) AS ssn_token, ssn AS ssn_original, name, region\n",
    "    FROM {table_name(RETAIL_SCHEMA, 'customers')}\"\"\")\n",
    "else:\n",
    "    spark.sql(f\"\"\"CREATE OR REPLACE {get_table_type()} TABLE {table_name(RETAIL_SCHEMA, 'customers_tokenized')} AS\n",
    "    SELECT id, {func_name('tokenize')}(ssn) AS ssn_token, ssn AS ssn_original, name, region\n",
    "    FROM {table_name(RETAIL_SCHEMA, 'customers')}\"\"\")\n",
    "\n",
    "# Create view with conditional detokenization\n",
    "if USE_TEMP_TABLES:\n",
    "    spark.sql(f\"\"\"CREATE OR REPLACE {get_table_type()} VIEW {view_name(RETAIL_SCHEMA, 'v_customers_tokenized')} AS\n",
    "    SELECT id, {func_name('detokenize')}(ssn_token, ssn_original) AS ssn, name, region\n",
    "    FROM customers_tokenized\"\"\")\n",
    "else:\n",
    "    spark.sql(f\"\"\"CREATE OR REPLACE {get_table_type()} VIEW {view_name(RETAIL_SCHEMA, 'v_customers_tokenized')} AS\n",
    "    SELECT id, {func_name('detokenize')}(ssn_token, ssn_original) AS ssn, name, region\n",
    "    FROM {table_name(RETAIL_SCHEMA, 'customers_tokenized')}\"\"\")\n",
    "\n",
    "print(\"Tokenized Storage:\")\n",
    "display(spark.sql(f\"SELECT id, ssn_token, name, region FROM {table_name(RETAIL_SCHEMA, 'customers_tokenized')} LIMIT 3\"))\n",
    "\n",
    "print(\"\\nConditional Detokenization:\")\n",
    "display(spark.sql(f\"SELECT * FROM {view_name(RETAIL_SCHEMA, 'v_customers_tokenized')} LIMIT 3\"))\n",
    "\n",
    "print(\"\\n‚úì Non-admins see tokens only\")\n",
    "print(\"‚úì Admins see original values\")\n",
    "print(\"\\nüîó Production: VGS ‚Ä¢ Basis Theory ‚Ä¢ TokenEx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2350b036-a143-4c16-8379-3cf257ea1f3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 7. Attribute-Based Access Control (ABAC)\n",
    "\n",
    "**What is ABAC?**\n",
    "Policy-driven access control based on object attributes (tags). Permissions set and enforced dynamically as data evolves.\n",
    "\n",
    "**Use Cases:**\n",
    "- Auto-deny access to columns tagged 'sensitivity=PII'\n",
    "- Monitor and protect credit card data automatically\n",
    "- Apply policies to new tables/columns with matching tags\n",
    "\n",
    "**Key Features:** Tag-based policies ‚Ä¢ Dynamic enforcement ‚Ä¢ Centralized governance\n",
    "\n",
    "**Status:** Currently in **Beta** (October 2025)\n",
    "\n",
    "[ABAC Documentation](https://docs.databricks.com/security/attribute-based-access-control.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27ebe8e1-bcdd-4c60-b314-73dcdc3a9551",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ABAC Conceptual Workflow (requires Beta workspace configuration)\n",
    "\n",
    "print(\"ABAC Workflow (Conceptual)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"Step 1: Tag column as PII\")\n",
    "print(\"  ALTER TABLE hr.employee_info\")\n",
    "print(\"  ALTER COLUMN ssn SET TAGS ('sensitivity' = 'PII');\\n\")\n",
    "\n",
    "print(\"Step 2: Create policy to mask PII\")\n",
    "print(\"  CREATE POLICY mask_pii ON SCHEMA hr\")\n",
    "print(\"  COLUMN MASK (ssn) USING '***-**-****'\")\n",
    "print(\"  TO all_accounts EXCEPT hr_admins;\\n\")\n",
    "\n",
    "print(\"Step 3: Apply policy\")\n",
    "print(\"  ALTER TABLE hr.employee_info\")\n",
    "print(\"  ALTER COLUMN ssn SET MASK POLICY mask_pii;\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Simulate with view\n",
    "spark.sql(f\"\"\"CREATE OR REPLACE {get_table_type()} VIEW {view_name(HR_SCHEMA, 'v_employee_info_abac')} AS\n",
    "SELECT id, name, salary,\n",
    "    CASE WHEN IS_ACCOUNT_GROUP_MEMBER('hr_admin') THEN ssn ELSE '***-**-****' END AS ssn\n",
    "FROM {table_name(HR_SCHEMA, 'employee_info')}\"\"\")\n",
    "\n",
    "print(\"Simulated ABAC Behavior:\")\n",
    "display(spark.sql(f\"SELECT * FROM {view_name(HR_SCHEMA, 'v_employee_info_abac')}\"))\n",
    "\n",
    "print(\"\\n‚úì Policy auto-masks columns tagged as PII\")\n",
    "print(\"‚úì Applies to all tables in schema\")\n",
    "print(\"‚úì Exceptions for specific groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b08d7580-a306-44f8-8cb9-6fdace2a21ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 8. Data Encryption\n",
    "\n",
    "**What is Encryption?**\n",
    "Protects data at rest and in transit by converting to encoded format readable only with decryption keys.\n",
    "\n",
    "**Databricks Encryption Options:**\n",
    "\n",
    "1. **AES Functions** - Column-level encryption (`AES_ENCRYPT`, `AES_DECRYPT`)\n",
    "2. **Server-side** - Automatic cloud storage encryption (S3, Azure Blob, GCS)\n",
    "3. **Format-Preserving** - Encrypt while maintaining format\n",
    "4. **Envelope Encryption** - Multi-layer DEK/KEK approach\n",
    "5. **Multi-key Protection** - Customer + Databricks managed keys\n",
    "\n",
    "**See `data_encryption.ipynb` for detailed encryption demonstrations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c0ac49f-a4a6-43e3-b34f-2744b16ab699",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"ssn\":194},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762798569114}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# AES-128 Encryption Demo\n",
    "encryption_key = \"MySecureKey12345\"  # ‚ö†Ô∏è Use Azure Key Vault/AWS KMS/GCP KMS in production\n",
    "\n",
    "# Create encrypted data as view or table\n",
    "if USE_TEMP_TABLES:\n",
    "    spark.sql(f\"\"\"CREATE OR REPLACE TEMP VIEW employee_info_encrypted AS\n",
    "    SELECT id, name, salary, base64(aes_encrypt(ssn, '{encryption_key}', 'ECB', 'PKCS')) AS ssn_encrypted\n",
    "    FROM employee_info\"\"\")\n",
    "    encrypted_table_ref = \"employee_info_encrypted\"\n",
    "else:\n",
    "    spark.sql(f\"\"\"CREATE OR REPLACE TABLE {table_name(HR_SCHEMA, 'employee_info_encrypted')} AS\n",
    "    SELECT id, name, salary, base64(aes_encrypt(ssn, '{encryption_key}', 'ECB', 'PKCS')) AS ssn_encrypted\n",
    "    FROM {table_name(HR_SCHEMA, 'employee_info')}\"\"\")\n",
    "    encrypted_table_ref = table_name(HR_SCHEMA, 'employee_info_encrypted')\n",
    "\n",
    "print(\"Encrypted Data:\")\n",
    "display(spark.sql(f\"SELECT * FROM {encrypted_table_ref}\"))\n",
    "\n",
    "# Create view with conditional decryption\n",
    "if USE_TEMP_TABLES:\n",
    "    spark.sql(f\"\"\"CREATE OR REPLACE TEMP VIEW v_employee_info_decrypted AS\n",
    "    SELECT id, name, salary,\n",
    "        CASE\n",
    "            WHEN IS_ACCOUNT_GROUP_MEMBER('hr_admin')\n",
    "            THEN aes_decrypt(unbase64(ssn_encrypted), '{encryption_key}', 'ECB', 'PKCS')\n",
    "            ELSE '***-**-****'\n",
    "        END AS ssn\n",
    "    FROM employee_info_encrypted\"\"\")\n",
    "    decrypted_view_ref = \"v_employee_info_decrypted\"\n",
    "else:\n",
    "    spark.sql(f\"\"\"CREATE OR REPLACE VIEW {view_name(HR_SCHEMA, 'v_employee_info_decrypted')} AS\n",
    "    SELECT id, name, salary,\n",
    "        CASE\n",
    "            WHEN IS_ACCOUNT_GROUP_MEMBER('hr_admin')\n",
    "            THEN aes_decrypt(unbase64(ssn_encrypted), '{encryption_key}', 'ECB', 'PKCS')\n",
    "            ELSE '***-**-****'\n",
    "        END AS ssn\n",
    "    FROM {table_name(HR_SCHEMA, 'employee_info_encrypted')}\"\"\")\n",
    "    decrypted_view_ref = view_name(HR_SCHEMA, 'v_employee_info_decrypted')\n",
    "\n",
    "print(\"\\nConditionally Decrypted:\")\n",
    "display(spark.sql(f\"SELECT * FROM {decrypted_view_ref}\"))\n",
    "\n",
    "print(\"\\n‚úì HR admins see decrypted values\")\n",
    "print(\"‚úì Others see masked values\")\n",
    "print(\"\\nüîí Best Practices:\")\n",
    "print(\"   ‚Ä¢ Use customer-managed keys (CMK)\")\n",
    "print(\"   ‚Ä¢ Rotate keys regularly\")\n",
    "print(\"   ‚Ä¢ Store keys in vault services (Key Vault, KMS)\")\n",
    "print(\"   ‚Ä¢ Enable TLS/SSL for data in transit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83c303f0-5ed7-43d3-a230-2fd425bd868c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Summary: Privacy Features Comparison\n",
    "\n",
    "| Feature | Use Case | Reversible | Performance | Complexity |\n",
    "|---------|----------|------------|-------------|------------|\n",
    "| **RBAC** | Role-based permissions | N/A | Low | Low |\n",
    "| **Views** | Controlled exposure | N/A | Low-Med | Low |\n",
    "| **Hashing** | Anonymization | No | Low | Low |\n",
    "| **Masking** | Format-preserving obfuscation | Optional | Low-Med | Medium |\n",
    "| **Row Filtering** | Regional/attribute access | N/A | Medium | Medium |\n",
    "| **Tokenization** | Reversible PII protection | Yes | Med-High | High |\n",
    "| **ABAC** | Policy-driven control | N/A | Medium | Med-High |\n",
    "| **Encryption** | At-rest/transit protection | Yes | Low-Med | Medium |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "‚úì **Defense in Depth** - Combine techniques for comprehensive protection\n",
    "‚úì **Unity Catalog** - Centralized governance for all privacy controls\n",
    "‚úì **Serverless Compute** - Required for fine-grained controls\n",
    "‚úì **Audit & Compliance** - All controls logged and auditable\n",
    "‚úì **Performance** - Consider impact when implementing complex policies\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Unity Catalog](https://docs.databricks.com/data-governance/unity-catalog/index.html)\n",
    "- [Row & Column Filters](https://docs.databricks.com/security/privacy/row-and-column-filters.html)\n",
    "- [ABAC](https://docs.databricks.com/security/attribute-based-access-control.html)\n",
    "- [Encryption](https://docs.databricks.com/security/encryption/index.html)\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Configuration Notes\n",
    "\n",
    "**Update these values for your Databricks environment:**\n",
    "\n",
    "**Group Names:**\n",
    "- `admin` ‚Üí Your admin group\n",
    "- `hr_admin` / `hr_viewer_group` ‚Üí Your HR groups\n",
    "- `Team_US` / `Team_EU` / `Team_APAC` ‚Üí Your regional groups\n",
    "\n",
    "**Encryption:**\n",
    "- Replace hardcoded keys with Azure Key Vault / AWS KMS / GCP KMS references\n",
    "\n",
    "**Catalogs/Schemas:**\n",
    "- Adjust in Configuration cell if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc7180c6-1fcc-40a2-88fd-a212cbd134da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Cleanup (Only for Permanent Tables)\n",
    "\n",
    "If you used permanent tables (`USE_TEMP_TABLES = False`), run the cleanup cell below.\n",
    "\n",
    "**Note:** Temporary tables are automatically cleaned up when your session ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7288d69-b1c4-4bda-8edf-b8af10db3640",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup (only runs if using permanent tables)\n",
    "if not USE_TEMP_TABLES:\n",
    "    print(\"Cleaning up permanent tables, views, functions...\")\n",
    "    \n",
    "    # Drop views\n",
    "    for view in ['employee_info_public', 'v_employee_info_abac', 'v_employee_info_decrypted']:\n",
    "        spark.sql(f\"DROP VIEW IF EXISTS {CATALOG}.{HR_SCHEMA}.{view}\")\n",
    "    for view in ['v_customers_private']:\n",
    "        spark.sql(f\"DROP VIEW IF EXISTS {CATALOG}.{CUSTOMERS_SCHEMA}.{view}\")\n",
    "    for view in ['v_customers_masked', 'v_customers_filtered', 'v_customers_tokenized']:\n",
    "        spark.sql(f\"DROP VIEW IF EXISTS {CATALOG}.{RETAIL_SCHEMA}.{view}\")\n",
    "    \n",
    "    # Drop tables\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{HR_SCHEMA}.employee_info\")\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{HR_SCHEMA}.employee_info_encrypted\")\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{CUSTOMERS_SCHEMA}.customer_info\")\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{RETAIL_SCHEMA}.customers\")\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{RETAIL_SCHEMA}.customers_tokenized\")\n",
    "    \n",
    "    # Drop functions\n",
    "    spark.sql(f\"DROP FUNCTION IF EXISTS {CATALOG}.{GOVERNANCE_SCHEMA}.mask_ssn\")\n",
    "    spark.sql(f\"DROP FUNCTION IF EXISTS {CATALOG}.{GOVERNANCE_SCHEMA}.filter_by_region\")\n",
    "    spark.sql(f\"DROP FUNCTION IF EXISTS {CATALOG}.{GOVERNANCE_SCHEMA}.tokenize\")\n",
    "    spark.sql(f\"DROP FUNCTION IF EXISTS {CATALOG}.{GOVERNANCE_SCHEMA}.detokenize\")\n",
    "    \n",
    "    print(\"‚úì Tables, views, and functions dropped\")\n",
    "    \n",
    "    # Drop schemas and catalog if created by this notebook\n",
    "    if CREATE_CATALOG_SCHEMA:\n",
    "        print(\"Dropping schemas...\")\n",
    "        for schema in [HR_SCHEMA, CUSTOMERS_SCHEMA, RETAIL_SCHEMA, GOVERNANCE_SCHEMA]:\n",
    "            spark.sql(f\"DROP SCHEMA IF EXISTS {CATALOG}.{schema} CASCADE\")\n",
    "        print(\"‚úì Schemas dropped\")\n",
    "        \n",
    "        print(\"Dropping catalog...\")\n",
    "        spark.sql(f\"DROP CATALOG IF EXISTS {CATALOG} CASCADE\")\n",
    "        print(\"‚úì Catalog dropped\")\n",
    "    \n",
    "    print(\"‚úì Cleanup complete\")\n",
    "else:\n",
    "    print(\"Using temporary tables - no cleanup needed!\")\n",
    "    print(\"Tables will be automatically removed when session ends.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6282640280599908,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "data_privacy_v2",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
